{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import make_scorer, recall_score, log_loss\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "\n",
    "# Model explainability\n",
    "import shap\n",
    "import xgboost\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "#from lime.lime_tabular import LimeTabularExplainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Read\n",
    "df = pd.read_csv('/Users/jasonrobinson/Documents/Projects/Customer-Churn/data/telecom_customer_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing with Pandas Profiling\n",
    "import pandas_profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=df.columns.str.replace(\" \",\"\").str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Nulls\n",
    "\n",
    "since this dataset is from a telecommunication company and this company has basically two main serivces that are\n",
    "\n",
    "1- Phone Service </br> \n",
    "2- Internet Service<br>\n",
    "\n",
    "So it isnt necesary that every customer is availing both of the services, and this caused presence of nulls in the dataset\n",
    "\n",
    "So I have replaced nulls for the features that are related to phone service as \"No phone service\" for the customer that are just #### using internet services and simlarly for the features that give info related to internet services have been replaced by \"No internet #### Service\" for the customers that are just using phone services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.avgmonthlylongdistancecharges=df.avgmonthlylongdistancecharges.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.multiplelines=df.multiplelines.fillna('no phone service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_internet=['internettype','onlinesecurity','onlinebackup','deviceprotectionplan','premiumtechsupport','streamingtv',\n",
    "             'streamingmovies','streamingmusic','unlimiteddata']\n",
    "df[no_internet]=df[no_internet].fillna('no internet service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.avgmonthlygbdownload=df.avgmonthlygbdownload.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping features that are of no importance for the my objective, I have dropped geographical features after finding no correlation with other features, and also the columns like churn category and churn reason are out of the scope of machine learning model, they can be useful in exploratory analysis but EDA is not the focus of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['customerid','churncategory','churnreason','totalrefunds','zipcode','longitude','latitude','city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the biasness in the predictions i have dropped the customers information that joined the company recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[~df.customerstatus.str.contains('Join')]\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = [\"No\", \"yes\"]\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "fig.add_trace(go.Pie(labels=type_, values=df['customerstatus'].value_counts(), name=\"customerstatus\"))\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", textfont_size=16)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Churn Distributions\",\n",
    "    # Add annotations in the center of the donut pies.\n",
    "    annotations=[dict(text='Churn', x=0.5, y=0.5, font_size=20, showarrow=False)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.customerstatus[df.customerstatus == 'Stayed'].groupby(by = df.gender).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.customerstatus[df.customerstatus == 'Churned'].groupby(by = df.gender).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"customerstatus\", color = \"contract\", barmode = \"group\", title = \"<b>Customer contract distribution<b>\")\n",
    "fig.update_layout(width=700, height=500, bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformation and Feature Scaling\n",
    "\n",
    "1- Features having two uniques were replaced by 1 and 0.<br>\n",
    "2- Features having more than two uniques were encoded using label encoder<br>\n",
    "3- Continous features were standarized using sk-learn scaler method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "# Label Encoding will be used for columns with 2 or less unique \n",
    "\n",
    "le_count = 0\n",
    "for col in df.columns[1:]:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            le.fit(df[col])\n",
    "            df[col] = le.transform(df[col])\n",
    "            le_count += 1\n",
    "print('{} columns were label encoded.'.format(le_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = [1 if each == 'Female' else 0 for each in df['gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(dataframe):\n",
    "    if dataframe.dtype == \"object\":\n",
    "        dataframe = LabelEncoder().fit_transform(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "data = df.apply(lambda x: encode_data(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = \"customerstatus\")\n",
    "y = data[\"customerstatus\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['totalcharges','avgmonthlylongdistancecharges','monthlycharge','totalrevenue','totallongdistancecharges',\n",
    "     'tenureinmonths','totallongdistancecharges','totalextradatacharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[col] = StandardScaler().fit_transform(X_train[col])\n",
    "X_test[col] = StandardScaler().fit_transform(X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building Without Optimization\n",
    "\n",
    "Since its a binary Classification Problem. So I have tried to build several classification models\n",
    "At first i have just used the base models and have evaluated them to check how they are pwrforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "models.append(('Kernel SVM', SVC()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Gaussian NB', GaussianNB()))\n",
    "models.append(('Random Forest', RandomForestClassifier()))\n",
    "models.append(('Decision Tree Classifier', DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results =[]\n",
    "auc_results =[]\n",
    "pre_results =[]\n",
    "f1_results =[]\n",
    "names = []\n",
    "\n",
    "result_col = [\"Algorithm\", \"ROC AUC\", \"Accuracy\", \"Precision\", \"f1 Score\"]\n",
    "model_results = pd.DataFrame(columns = result_col)\n",
    "\n",
    "i=0\n",
    "# K- fold cross validation\n",
    "\n",
    "for name, model in models:\n",
    "    names.append(name)\n",
    "    # kfold = model_selection.KFold(n_splits=10)\n",
    "    \n",
    "    cv_acc_results = model_selection.cross_val_score(model, X_train, y_train, \n",
    "                     scoring=\"accuracy\")\n",
    "    cv_auc_results = model_selection.cross_val_score(model, X_train, y_train,\n",
    "                     scoring=\"roc_auc\")\n",
    "    cv_pre_results = model_selection.cross_val_score(model, X_train, y_train,\n",
    "                     scoring=\"precision\")\n",
    "    cv_f1_results = model_selection.cross_val_score(model, X_train, y_train,\n",
    "                     scoring=\"f1\")\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    pre_results.append(cv_pre_results)\n",
    "    f1_results.append(cv_f1_results)\n",
    "    \n",
    "    model_results.loc[i] = [name, \n",
    "                           round(cv_acc_results.mean()*100,2),\n",
    "                           round(cv_auc_results.mean()*100,2),\n",
    "                           round(cv_pre_results.mean()*100,2),\n",
    "                           round(cv_f1_results.mean()*100,2)]\n",
    "    i+=1\n",
    "\n",
    "model_results.sort_values(by = ['ROC AUC'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_ID = modelD.predict(X_train)\n",
    "pred_test_ID = modelD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_train, pred_train_ID)\n",
    "acc_test = accuracy_score(y_test, pred_test_ID)\n",
    "print(f'Traning Accuracy: {acc_train}')\n",
    "print(f'Testing Accuracy: {acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'criterion':['gini', 'entropy'],\n",
    "                     'max_leaf_nodes': range(5,25)}] # hyperparameters to tune\n",
    "\n",
    "clf_D = GridSearchCV(DecisionTreeClassifier(), tuned_parameters, \n",
    "                   verbose=1, n_jobs=-1) # grid search model\n",
    "clf_D.fit(X_train, y_train) # evaluate hyper-parameters\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(clf_D.best_params_) # best hyperparameter balues\n",
    "\n",
    "print(\"\\nGrid scores:\")\n",
    "means_D = clf_D.cv_results_['mean_test_score'] # mean accuracy with folds\n",
    "stds_D = clf_D.cv_results_['std_test_score'] # standard deviation of accuracies\n",
    "# for each hyperparameter combination show mean +/- 2 standard-deviations \n",
    "for mean, std, params in zip(means_D, stds_D, clf_D.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" %(mean, std * 2, params)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Decision Tree classifier with selected hyper-parameters\n",
    "Based on our 5-fold-cross-validation, we use a model with the following hyper-parameters:\n",
    "\n",
    "criterion = 'gini'\n",
    "max_leaf_nodes = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'gini'\n",
    "max_leaf_nodes = 19\n",
    "# Train and test model\n",
    "good_model_D = DecisionTreeClassifier(criterion=criterion,\n",
    "                                    max_leaf_nodes=max_leaf_nodes) # create model \n",
    "print(good_model_D) # display model parameters\n",
    "good_model_D.fit(X_train, y_train) # train model\n",
    "pred_D = good_model_D.predict(X_test) # predicted output for test examples\n",
    "print(\"Results on test data\")\n",
    "acc_D = accuracy_score(y_test, pred_D) # accuracy on test examples\n",
    "prec_D = precision_score(y_test, pred_D) # precision on test examples\n",
    "reca_D = recall_score(y_test, pred_D) # recall on test examples\n",
    "print(f'Test accuracy = {acc_D: .4f}') # round to 4 decimal places\n",
    "print(f'Test precision = {prec_D: .4f}') # round to 4 decimal places\n",
    "print(f'Test recall = {reca_D: .4f}') # round to 4 decimal places\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, pred_D))\n",
    "print(\"Confusion matrix (Rows actual, Columns predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred_D)))\n",
    "print('\\nROC curve')\n",
    "plot_roc_curve(good_model_D, X_test, y_test)  # \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forset Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelR = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = modelR.predict(X_train)\n",
    "pred_test = modelR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(pred_train, y_train)\n",
    "acc_test = accuracy_score(pred_test, y_test)\n",
    "print(f'Training accuracy {acc_train: .3f}') \n",
    "print(f'Testing accuracy {acc_test: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'n_estimators': [25, 50, 75],\n",
    "                     'max_features': [15, 20, 25],\n",
    "                     'max_leaf_nodes': [8, 16, 24]}] # hyperparameters to tune\n",
    "\n",
    "clfR = GridSearchCV(RandomForestClassifier(), tuned_parameters, \n",
    "                   verbose=1, n_jobs=-1) # grid search model\n",
    "clfR.fit(X_train, y_train) # evaluate hyper-parameters\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(clfR.best_params_) # best hyperparameter values\n",
    "\n",
    "print(\"\\nGrid scores:\")\n",
    "means = clfR.cv_results_['mean_test_score'] # mean accuracy with folds\n",
    "stds = clfR.cv_results_['std_test_score'] # standard deviation of accuracies\n",
    "# for each hyperparameter combination show mean +/- 2 standard-deviations \n",
    "for mean, std, params in zip(means, stds, clfR.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" %(mean, std * 2, params)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Evaluating Random forest classifier with selected hyper-parameters\n",
    "Based on our 5-fold-cross-validation, we use a model with the following hyper-parameters:\n",
    "\n",
    "max_features = '15'\n",
    "max_leaf_nodes = 16\n",
    "n-estimators = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify non-default hyper-parameter values\n",
    "max_features = 15\n",
    "max_leaf_nodes = 16 \n",
    "n_estimators = 50\n",
    "# Train and test model\n",
    "good_model = RandomForestClassifier(max_leaf_nodes = max_leaf_nodes,\n",
    "                                    max_features = max_features, \n",
    "                                    n_estimators=n_estimators, ) # create model \n",
    "print(good_model) # display model parameters\n",
    "good_model.fit(X_train, y_train) # train model\n",
    "pred = good_model.predict(X_test) # predicted output for test examples\n",
    "print(\"Results on test data\")\n",
    "acc = accuracy_score(y_test, pred) # accuracy on test examples\n",
    "prec = precision_score(y_test, pred) # precision on test examples\n",
    "reca = recall_score(y_test, pred) # recall on test examples\n",
    "print(f'Test accuracy = {acc: .4f}') # round to 4 decimal places\n",
    "print(f'Test precision = {prec: .4f}') # round to 4 decimal places\n",
    "print(f'Test recall = {reca: .4f}') # round to 4 decimal places\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion matrix (Rows actual, Columns predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred)))\n",
    "print('\\nROC curve')\n",
    "plot_roc_curve(good_model, X_test, y_test)  # \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipped some model, but will go back and demonstrate at a later time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on the basis of best Parameters\n",
    "# Specify non-default hyper-parameter values\n",
    "C = 1 # algorithm name\n",
    "kernel = 'linear'# kernel type\n",
    "gamma =  0.1 # kernel parameter\n",
    "# Train and test model\n",
    "good_modelsvm = svm.SVC(C=C, kernel=kernel, \n",
    "                                  gamma=gamma) # create model \n",
    "print(good_modelsvm) # display model parameters\n",
    "good_modelsvm.fit(X_train,y_train) # train model\n",
    "predsvm = good_modelsvm.predict(X_test) # predicted output for test examples\n",
    "print(\"Results on test data\")\n",
    "accsvm = accuracy_score(y_test, predsvm) # accuracy on test examples\n",
    "precsvm = precision_score(y_test, predsvm) # precision on test examples\n",
    "recasvm = recall_score(y_test, predsvm) # recall on test examples\n",
    "print(f'Test accuracy = {accsvm: .4f}') # round to 4 decimal places\n",
    "print(f'Test precision = {precsvm: .4f}') # round to 4 decimal places\n",
    "print(f'Test recall = {recasvm: .4f}') # round to 4 decimal places\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, predsvm))\n",
    "print(\"Confusion matrix (Rows actual, Columns predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predsvm)))\n",
    "print('\\nROC curve')\n",
    "plot_roc_curve(good_modelsvm, X_test, y_test)  # \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_opt = []\n",
    "\n",
    "models_opt.append(('Logistic Regression', LogisticRegression(solver='liblinear',C = 1000, max_iter=1000, random_state = 0)))\n",
    "models_opt.append(('SVC', SVC(C=1, kernel = 'linear', gamma=1, random_state = 0)))\n",
    "models_opt.append(('Kernel SVM', SVC(C=1, kernel = 'rbf', gamma=1, random_state = 0)))\n",
    "models_opt.append(('KNN', KNeighborsClassifier(n_neighbors = 20, metric = 'euclidean', p = 2,algorithm='brute')))\n",
    "models_opt.append(('Gaussian NB', GaussianNB()))\n",
    "models_opt.append(('Decision Tree Classifier', DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes=19, random_state = 0)))\n",
    "models_opt.append(('Random Forest', RandomForestClassifier(max_leaf_nodes = 16,max_features = 15, \n",
    "                                                           n_estimators = 50,criterion = 'entropy', random_state = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results_opt =[]\n",
    "auc_results_opt =[]\n",
    "pre_results_opt =[]\n",
    "f1_results_opt =[]\n",
    "names_opt = []\n",
    "\n",
    "result_col_opt = [\"Algorithm\", \"ROC AUC\", \"Accuracy\", 'Precision', 'F1 Scores']\n",
    "model_results_opt = pd.DataFrame(columns = result_col_opt)\n",
    "\n",
    "i=0\n",
    "# K- fold cross validation\n",
    "\n",
    "for name, model in models:\n",
    "    names_opt.append(name)\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    \n",
    "    cv_acc_results_opt = model_selection.cross_val_score(model, X_train, y_train, \n",
    "                    cv = kfold, scoring=\"accuracy\")\n",
    "    cv_auc_results_opt = model_selection.cross_val_score(model, X_train, y_train,\n",
    "                    cv = kfold, scoring=\"roc_auc\")\n",
    "    cv_pre_results_opt = model_selection.cross_val_score(model, X_train, y_train,\n",
    "                    cv = kfold, scoring=\"precision\")\n",
    "    cv_f1_results_opt = model_selection.cross_val_score(model, X_train, y_train,\n",
    "                    cv = kfold, scoring=\"f1\")\n",
    "    acc_results_opt.append(cv_acc_results_opt)\n",
    "    auc_results_opt.append(cv_auc_results_opt)\n",
    "    pre_results_opt.append(cv_pre_results_opt)\n",
    "    f1_results_opt.append(cv_f1_results_opt)\n",
    "    model_results_opt.loc[i] = [name, \n",
    "                           round(cv_auc_results_opt.mean()*100,2),\n",
    "                           round(cv_acc_results_opt.mean()*100,2),\n",
    "                           round(cv_pre_results_opt.mean()*100,2),\n",
    "                           round(cv_f1_results_opt.mean()*100,2)]\n",
    "    i+=1\n",
    "\n",
    "model_results_opt.sort_values(by = ['ROC AUC'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results_opt =[]\n",
    "auc_results_opt =[]\n",
    "pre_results_opt =[]\n",
    "f1_results_opt =[]\n",
    "names_opt = []\n",
    "\n",
    "result_col_opt = [\"Algorithm\", \"ROC AUC\", \"Accuracy\", 'Precision', 'F1 Scores']\n",
    "model_results_opt = pd.DataFrame(columns = result_col_opt)\n",
    "\n",
    "i=0\n",
    "# K- fold cross validation\n",
    "\n",
    "for name, model in models:\n",
    "    names_opt.append(name)\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    \n",
    "    cv_acc_results_opt = model_selection.cross_val_score(model, X_test, y_test, \n",
    "                    cv = kfold, scoring=\"accuracy\")\n",
    "    cv_auc_results_opt = model_selection.cross_val_score(model, X_test, y_test,\n",
    "                    cv = kfold, scoring=\"roc_auc\")\n",
    "    cv_pre_results_opt = model_selection.cross_val_score(model, X_test, y_test,\n",
    "                    cv = kfold, scoring=\"precision\")\n",
    "    cv_f1_results_opt = model_selection.cross_val_score(model, X_test, y_test,\n",
    "                    cv = kfold, scoring=\"f1\")\n",
    "    acc_results_opt.append(cv_acc_results_opt)\n",
    "    auc_results_opt.append(cv_auc_results_opt)\n",
    "    pre_results_opt.append(cv_pre_results_opt)\n",
    "    f1_results_opt.append(cv_f1_results_opt)\n",
    "    model_results_opt.loc[i] = [name, \n",
    "                           round(cv_auc_results_opt.mean()*100,2),\n",
    "                           round(cv_acc_results_opt.mean()*100,2),\n",
    "                           round(cv_pre_results_opt.mean()*100,2),\n",
    "                           round(cv_f1_results_opt.mean()*100,2)]\n",
    "    i+=1\n",
    "\n",
    "model_results_opt.sort_values(by = ['ROC AUC'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(good_model_D, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d = data.query('monthlycharge >= 0')\n",
    "y1_d = data.customerstatus\n",
    "base_features_d=['tenureinmonths','contract','numberofreferrals','age','monthlycharge']\n",
    "X1_d = data[base_features_d]\n",
    "train_X1_d, val_X1_d, train_y1_d, val_y1_d = train_test_split(X1_d, y1_d, random_state=1)\n",
    "good_model_D1 = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=19, random_state = 0).fit(train_X1_d, train_y1_d)\n",
    "print(\"Data sample:\")\n",
    "data_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name in base_features_d:\n",
    "    pdp_dist =  pdp.pdp_isolate(model=good_model_D1, dataset=val_X1_d,\n",
    "                               model_features=base_features_d, feature=feat_name)\n",
    "    pdp.pdp_plot(pdp_dist, feat_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(good_model_D)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(good_model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "data1 = data.query('monthlycharge >= 0')\n",
    "y1 = data.customerstatus\n",
    "base_features=['tenureinmonths','contract','numberofreferrals','age','numberofdependents','monthlycharge']\n",
    "X1 = data[base_features]\n",
    "train_X1, val_X1, train_y1, val_y1 = train_test_split(X1, y1, random_state=1)\n",
    "first_model = RandomForestRegressor(n_estimators=50, random_state=1).fit(train_X1, train_y1)\n",
    "print(\"Data sample:\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name in base_features:\n",
    "    pdp_dist =  pdp.pdp_isolate(model=first_model, dataset=val_X1,\n",
    "                               model_features=base_features, feature=feat_name)\n",
    "    pdp.pdp_plot(pdp_dist, feat_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model.predict(np.array(X_test.values[0]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = ['tenureinmonths', 'contract']\n",
    "partial_plot  =  pdp.pdp_interact(model=first_model, dataset=val_X1,\n",
    "                                  model_features=base_features, features=fnames)\n",
    "pdp.pdp_interact_plot(pdp_interact_out=partial_plot,\n",
    "                      feature_names=fnames, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = ['contract', 'tenureinmonths']\n",
    "# Specify non-default hyper-parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = ['contract', 'tenureinmonths']\n",
    "# Specify non-default hyper-parameter values\n",
    "max_features = 15\n",
    "max_leaf_nodes = 16 \n",
    "n_estimators = 50\n",
    "# Train and test model\n",
    "good_model = RandomForestClassifier(max_leaf_nodes = max_leaf_nodes,\n",
    "                                    max_features = max_features, \n",
    "                                    n_estimators=n_estimators, ) # create model \n",
    "print(good_model) # display model parameters\n",
    "good_model.fit(X_train, y_train) # train model\n",
    "pred = good_model.predict(X_test) # predicted output for test examples\n",
    "print(\"Results on test data\")\n",
    "acc = accuracy_score(y_test, pred) # accuracy on test examples\n",
    "prec = precision_score(y_test, pred) # precision on test examples\n",
    "reca = recall_score(y_test, pred) # recall on test examples\n",
    "print(f'Test accuracy = {acc: .4f}') # round to 4 decimal places\n",
    "print(f'Test precision = {prec: .4f}') # round to 4 decimal places\n",
    "print(f'Test recall = {reca: .4f}') # round to 4 decimal places\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion matrix (Rows actual, Columns predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred)))\n",
    "print('\\nROC curve')\n",
    "plot_roc_curve(good_model, X_test, y_test)  # \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(good_modelL, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = data.customerstatus\n",
    "base_features2=['tenureinmonths','contract','numberofreferrals','married','totalcharges']\n",
    "X2 = data[base_features]\n",
    "train_X2, val_X2, train_y2, val_y2 = train_test_split(X2, y, random_state=1)\n",
    "second_model= LogisticRegression(C=1000, max_iter= 1000).fit(train_X2, train_y2)\n",
    "print(\"Data sample:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name in base_features:\n",
    "    pdp_dist =  pdp.pdp_isolate(model=second_model, dataset=val_X2,\n",
    "                               model_features=base_features, feature=feat_name)\n",
    "    pdp.pdp_plot(pdp_dist, feat_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = ['contract', 'tenureinmonths', 'numberofreferrals','married']\n",
    "model_impL = LogisticRegression(C=1000, max_iter= 1000) # model with important features\n",
    "model_impL.fit(X_train[imp_features], y_train)\n",
    "pred = model_impL.predict(X_train[imp_features]) # predicted output for test examples\n",
    "print(\"Results on test data\")\n",
    "accL = accuracy_score(y_test, predL) # accuracy on test examples\n",
    "precL = precision_score(y_test, predL) # precision on test examples\n",
    "recaL = recall_score(y_test, predL) # recall on test examples\n",
    "print(f'Test accuracy = {accL: .4f}') # round to 4 decimal places\n",
    "print(f'Test precision = {precL: .4f}') # round to 4 decimal places\n",
    "print(f'Test recall = {recaL: .4f}') # round to 4 decimal places\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, predL))\n",
    "print(\"Confusion matrix (Rows actual, Columns predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predL)))\n",
    "print('\\nROC curve')\n",
    "plot_roc_curve(model_impL, X_test[imp_features], y_test)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(good_modelknn, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(good_modelsvm, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "SIX MACHINE LEARNING MODELS WERE BUILT AND TRAINED\n",
    "\n",
    "RANDOM FOREST OUTPERFORMED OTHER MODELS\n",
    "\n",
    "MOST IMPORTANT FEATURES ARE\n",
    "\n",
    "    CONTRACT\n",
    "    MONTHLY CHARGE\n",
    "    TENURE IN MONTHS\n",
    "    NUMBER OF REFERRALS\n",
    "    NUMBER OF INDEPENDENTS\n",
    "linkcode\n",
    "Further Working can be done in model explanation using LIME and SHAP methods that build more trust and reliability on the above used models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4ae6f0424a16e8e91b0ab48073d90889a15ac39d212e16aef0dd7af596a8749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
